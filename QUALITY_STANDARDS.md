# Java SWE-Bench Quality Standards

## ğŸ¯ Overview

This document defines the **strict quality standards** for task inclusion in the Java SWE-Bench benchmark. We only accept **high-quality, real-world tasks** that represent genuine software engineering challenges.

## âœ¨ Quality Philosophy

### What We Want
âœ… **Real bugs** that developers actually encountered
âœ… **Meaningful features** that add genuine value
âœ… **Production-grade code** with proper tests
âœ… **Clear problem statements** with reproduction steps
âœ… **Focused fixes** that solve specific issues
âœ… **Stable tests** that reliably pass/fail

### What We Reject
âŒ **Trivial changes** (typos, formatting, comments)
âŒ **Documentation-only** updates
âŒ **Build script** tweaks
âŒ **Dependency version** bumps
âŒ **Style/linting** fixes
âŒ **Unclear issues** without proper context
âŒ **Flaky tests** that are non-deterministic

---

## ğŸ“‹ Quality Validation Stages

### Stage 1: Repository Quality (Discovery)

**Criteria:**
- âœ… Java â‰¥ 75% of codebase (ensure Java-centric)
- âœ… 50+ GitHub stars (popularity indicator)
- âœ… Not forked (original work)
- âœ… Updated within 1-2 years (active maintenance)
- âœ… Has active issues and merged PRs
- âœ… Includes comprehensive test suite
- âœ… Uses Maven or Gradle

**Purpose:** Ensure we're working with high-quality, well-maintained projects

---

### Stage 2: Task Quality (Attribute Filtering)

#### A. Problem Statement Quality

**REQUIRED (Minimum 100 characters):**
```
âŒ BAD: "Fix bug"
âŒ BAD: "Update method"
âœ… GOOD: "Observable.concat fails with NullPointerException when
         called with empty sequence. Expected behavior is to
         return empty observable. Occurs in RxJava 2.x when..."
```

**Quality Checks:**
- Minimum 100 characters (configurable)
- At least 2 sentences
- Contains context words: "when", "expected", "actual", "reproduce", "steps"
- Not just a title, actual description

**Automatic Rejection:**
- One-liner issues
- Title-only problems
- No context provided

#### B. Real-World Issue Detection

**HIGH-QUALITY INDICATORS (at least ONE required):**
```
âœ… bug, error, exception, crash, fail
âœ… incorrect, wrong, invalid, broken
âœ… memory leak, performance issue
âœ… deadlock, race condition, concurrent
âœ… security vulnerability
âœ… data loss, corruption, inconsistent
âœ… regression, not working
âœ… feature request, enhancement, implement
```

**LOW-QUALITY PATTERNS (AUTO-REJECT):**
```
âŒ typo, spelling, grammar, formatting
âŒ whitespace, indentation, style
âŒ readme, documentation only, doc update
âŒ comment, javadoc only
âŒ dependency version, upgrade dependency
âŒ add license, copyright
âŒ gitignore, travis, ci config
```

**Example Validation:**
```java
âœ… ACCEPTED: "NullPointerException when processing concurrent requests"
   â†’ Contains: "exception", "concurrent" (real bug)

âŒ REJECTED: "Fix typo in README.md"
   â†’ Contains: "typo", "README" (trivial doc change)

âŒ REJECTED: "Update Spring dependency from 5.1 to 5.2"
   â†’ Contains: "dependency" (version bump)

âœ… ACCEPTED: "Memory leak in connection pool causes OOM after 1000 requests"
   â†’ Contains: "memory leak", real performance bug
```

#### C. Patch Quality

**Size Requirements:**
- Minimum: 5 lines changed (substantive change)
- Maximum: 500 lines changed (focused fix)
- Must modify at least one .java file

**Content Requirements:**
âœ… Must include code changes (not just deletions)
âœ… Must affect Java source files
âœ… Must include test changes
âœ… Should have meaningful additions (not just whitespace)

**Automatic Rejections:**
```
âŒ Deletion-only patches (just removing code)
âŒ No Java file modifications
âŒ Only build file changes
âŒ Only documentation changes
âŒ Only whitespace/formatting
```

**Example:**
```diff
âŒ REJECTED (too small):
   - private int count = 0;
   + private int count = 1;

âœ… ACCEPTED (substantive):
   +    if (source == null) {
   +        throw new NullPointerException("Source cannot be null");
   +    }
   +
   +    synchronized (lock) {
   +        return doProcess(source);
   +    }

   // Plus corresponding test additions
```

#### D. Test Quality

**REQUIRED:**
- âœ… Modifies at least one test file
- âœ… Adds at least 3 test-related lines
- âœ… Includes assertions/verifications
- âœ… Test method names are meaningful

**Test Change Detection:**
```java
âœ… REQUIRED patterns in additions:
   - @Test annotation
   - assert* statements (assertTrue, assertEquals, etc.)
   - verify/expect/should methods
   - Test method names (testXyz, shouldXyz)
```

**Examples:**
```java
âŒ REJECTED (no assertions):
   @Test
   public void testFoo() {
       obj.doSomething();
   }

âœ… ACCEPTED (proper test):
   @Test
   public void testConcatWithNull_shouldThrowNPE() {
       Observable<String> obs = Observable.concat(null);

       assertThrows(NullPointerException.class, () -> {
           obs.subscribe();
       });
   }
```

#### E. Quality Scoring

**Score Breakdown (0-100):**
- **Problem Statement** (25 points)
  - Has meaningful content: +25
  - > 300 characters: +5
  - Contains "reproduce": +5

- **Real-World Relevance** (25 points)
  - Matches quality patterns: +25

- **Patch Quality** (25 points)
  - Well-formed patch: +25

- **Test Quality** (25 points)
  - Has quality tests: +20
  - Has valid fail-to-pass: +5

**Minimum Scores:**
- **75+**: ACCEPTED (GOOD quality)
- **90+**: EXCELLENT quality
- **<75**: REJECTED

---

### Stage 3: Execution Validation

#### A. Fail-to-Pass Verification (CRITICAL)

**The Gold Standard:**
```
1. Tests MUST fail at base commit
2. Tests MUST pass after patch
3. No other tests should break
4. Tests must be stable (not flaky)
```

**Validation Steps:**

**Step 1: Pre-Patch Test (MUST FAIL)**
```bash
git checkout <base_commit>
mvn test -Dtest=MyTest#testFoo

Expected: âŒ FAILURE
If PASS â†’ âš ï¸ FALSE POSITIVE, REJECT TASK
```

**Step 2: Apply Patch**
```bash
git apply patch.diff

Expected: âœ… Success
If fails â†’ âŒ Invalid patch, REJECT
```

**Step 3: Post-Patch Test (MUST PASS)**
```bash
mvn test -Dtest=MyTest#testFoo

Expected: âœ… SUCCESS
If FAIL â†’ âŒ Patch doesn't fix, REJECT
```

**Step 4: Regression Check (MUST STILL PASS)**
```bash
mvn test -Dtest=OtherTests

Expected: âœ… All PASS
If ANY FAIL â†’ âŒ Regression, REJECT
```

**Step 5: Stability Check (MUST BE CONSISTENT)**
```bash
# Run tests again
mvn test -Dtest=MyTest#testFoo

Expected: âœ… SUCCESS (consistent with step 3)
If FAIL â†’ âŒ Flaky test, REJECT
```

#### B. Fail-to-Pass Test Structure

**REQUIRED:**
- Must have 1-10 FAIL_TO_PASS tests
- Test names must be meaningful (>5 characters)
- Tests must be properly formatted

**Examples:**
```
âœ… GOOD:
   FAIL_TO_PASS: ["ObservableTest#testConcatWithNull"]

âŒ BAD:
   FAIL_TO_PASS: [] (empty)

âŒ BAD:
   FAIL_TO_PASS: ["test1", "test2", ..., "test50"] (too many)

âŒ BAD:
   FAIL_TO_PASS: ["t"] (not meaningful)
```

#### C. Pass-to-Pass Validation

**Purpose:** Ensure no regressions

**Process:**
```
For each test that PASSED at base commit:
   Run test after patch
   IF test now FAILS â†’ âŒ REGRESSION, REJECT TASK
```

**Why This Matters:**
- Ensures patch doesn't break existing functionality
- Validates fix is focused and targeted
- Prevents "fixing one bug by creating another"

---

## ğŸ“ Quality Examples

### âœ… EXCELLENT Task Example

**Issue:** "Memory leak in HTTP connection pool"

**Problem Statement:**
```
When making > 1000 HTTP requests, the connection pool fails to
release connections properly, leading to OutOfMemoryError.

Steps to reproduce:
1. Create HttpClient with connection pool
2. Make 1000+ requests
3. Monitor memory usage
4. Observe OOM after ~1500 requests

Expected: Connections released after use
Actual: Connections accumulate, causing OOM

Affects: HttpClient 4.5.x
Environment: Java 11, Linux
```

**Quality Score:** 95/100
- âœ… Detailed problem statement (500+ chars)
- âœ… Real-world bug (memory leak)
- âœ… Reproduction steps provided
- âœ… Clear expected vs actual
- âœ… Environment details

**Patch:** 35 lines
- Adds proper connection release logic
- Includes try-finally block
- Adds defensive null checks
- Updates 2 test files with 5 new test methods

**Tests:**
```java
@Test
public void testConnectionRelease_shouldCloseAfterUse() { ... }

@Test
public void testConnectionPool_shouldNotLeakMemory() { ... }
```

**Validation:**
- âœ… Tests fail at base (OOM occurs)
- âœ… Tests pass after patch (no leak)
- âœ… All other tests still pass
- âœ… Stable across multiple runs

---

### âŒ REJECTED Task Examples

#### Example 1: Trivial Typo
```
Issue: "Fix typo in variable name"
Problem: "Rename 'clas' to 'class'"
Patch: 1 line (renaming)
Reason: âŒ Trivial change, no real bug
```

#### Example 2: Documentation Only
```
Issue: "Update README with new examples"
Patch: Only modifies README.md (no code)
Reason: âŒ Documentation only, no code fix
```

#### Example 3: False Positive
```
Issue: "Tests are failing"
Problem: "Fix failing tests"
Patch: Modifies test expectations

Pre-patch test: âœ… PASS (no actual failure!)
Reason: âŒ False positive, tests already passing
```

#### Example 4: Flaky Tests
```
Issue: "Concurrent test occasionally fails"
Pre-patch: âŒ FAIL
Post-patch: âœ… PASS
Second run post-patch: âŒ FAIL
Reason: âŒ Flaky test, not deterministic
```

#### Example 5: Too Large
```
Issue: "Refactor entire authentication module"
Patch: 2000 lines, 50 files
Reason: âŒ Too large, not focused
```

---

## ğŸ“Š Expected Quality Metrics

### Input â†’ Output Funnel

```
Stage 1: Discovery
   Input: ~1000 repositories
   Output: 20-30 qualified repositories
   Pass Rate: ~2-3%

Stage 2: Attribute Filtering
   Input: ~1000 issue-PR pairs
   Output: ~400 candidate tasks
   Pass Rate: ~40%

Stage 3: Execution Validation
   Input: ~400 candidates
   Output: ~200 validated tasks
   Pass Rate: ~50%

Final Dataset:
   200+ high-quality tasks
   From 20-30 repositories
   Quality score: Average 85+/100
```

### Quality Distribution Goals

```
90-100 (EXCELLENT): 30% (60 tasks)
75-89  (GOOD):      60% (120 tasks)
60-74  (ACCEPTABLE): 10% (20 tasks)
<60    (REJECTED):  0%
```

---

## âš™ï¸ Configuration

### Quality Thresholds (application.properties)

```properties
# Repository Quality
discovery.min.java.percentage=75.0
discovery.min.stars=50

# Task Quality
filter.min.problem.statement.length=100
filter.min.quality.score=75
filter.require.real.world.issue=true
filter.max.files.changed=100

# Test Validation
execution.verify.stability=true
execution.require.pass.to.pass=true
execution.timeout.minutes=10
```

### Adjusting Standards

**To be MORE strict (higher quality):**
```properties
filter.min.quality.score=85
filter.min.problem.statement.length=200
discovery.min.java.percentage=90.0
```

**To be MORE lenient (more tasks):**
```properties
filter.min.quality.score=65
filter.min.problem.statement.length=75
discovery.min.java.percentage=60.0
```

**Recommendation:** Start with defaults (75/100), only adjust if:
- Not reaching 200 task target â†’ Lower to 70
- Too many low-quality tasks â†’ Raise to 80

---

## ğŸ” Quality Assurance Process

### Automated Validation

Every task goes through:
1. âœ… QualityValidator.isHighQuality()
2. âœ… QualityValidator.hasValidFailToPassTests()
3. âœ… Execution validation (5-step process)
4. âœ… Stability verification

### Manual Spot-Check (Recommended)

Sample 10% of validated tasks:
```bash
# Select 20 random tasks
for task in $(ls data/tasks/*.json | shuf | head -20); do
    # Review manually
    cat $task | jq '.problem_statement'
    cat $task | jq '.patch' | head -50
done
```

**Check for:**
- Clear problem description?
- Meaningful code change?
- Proper test coverage?
- Real-world relevance?

---

## ğŸ“ˆ Quality Metrics Dashboard

### Per-Repository Metrics

Track for each repository:
- Tasks extracted
- Tasks validated
- Average quality score
- Pass rate
- Common rejection reasons

### Overall Metrics

```
Total Repositories: 25
Total Tasks: 215
Average Quality Score: 87.3/100

Quality Distribution:
  EXCELLENT (90+): 72 tasks (33%)
  GOOD (75-89): 128 tasks (60%)
  ACCEPTABLE (60-74): 15 tasks (7%)

Validation Success Rate: 52%

Top Rejection Reasons:
  1. Low quality score: 35%
  2. False positive (tests pass at base): 25%
  3. Flaky tests: 20%
  4. Trivial changes: 15%
  5. Other: 5%
```

---

## ğŸ¯ Success Criteria

**Phase 2 Complete When:**

âœ… 200+ tasks validated
âœ… Average quality score â‰¥ 80/100
âœ… 100% pass fail-to-pass criterion
âœ… 0% regression (pass-to-pass maintained)
âœ… < 5% flaky tests
âœ… Tasks from 20+ repositories
âœ… Quality distribution matches goals

---

## ğŸš€ Running with Quality Standards

```bash
# Discovery with quality filters
./scripts/run_discovery.sh

# Attribute filtering with quality validation
./scripts/run_filter.sh

# Execution validation with stability checks
./scripts/run_validate.sh

# Full pipeline
./scripts/run_pipeline.sh
```

**Monitor quality:**
```bash
# Check quality scores
grep "quality:" logs/filter.log

# Check validation results
grep "âœ…\|âŒ" logs/validate.log

# Generate quality report
cat data/tasks/validated_tasks.json | jq '[.[].metadata.quality_score] | add/length'
```

---

## ğŸ“š References

- **Python SWE-bench paper**: Quality standards from original
- **Best practices**: Industry-standard bug reporting
- **Test-driven development**: Fail-to-pass methodology

---

## âœ¨ Conclusion

These quality standards ensure that Java SWE-Bench contains **only high-quality, real-world tasks** that genuinely test a model's software engineering capabilities.

**Remember:**
- Quality > Quantity
- Real bugs > Trivial changes
- Stable tests > Flaky tests
- Clear problems > Vague issues

**Goal:** Create a benchmark that developers respect and trust.
